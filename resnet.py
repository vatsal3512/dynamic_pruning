# -*- coding: utf-8 -*-
"""resent.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1sJRzgYJ0JggdyKmwCgsVNbiaPfLVIMl8
"""

from google.colab import drive
drive.mount('/content/drive',force_remount=True)

# Commented out IPython magic to ensure Python compatibility.
import torch.nn as nn
import torch
import torchvision
from torchvision import models, transforms
from torch.optim.lr_scheduler import ReduceLROnPlateau, StepLR

import numpy as np
import random

# %matplotlib inline
import matplotlib.pyplot as plt

from copy import deepcopy

basepath = '/content/drive/MyDrive/pruning test'

class EarlyStopping:
    def __init__(self, patience=7, filename='checkpoint_0', verbose=False):
        self.patience = patience
        self.verbose = verbose
        self.counter = 0
        self.best_score = None
        self.early_stop = False
        self.val_loss_min = np.inf
        self.filename = filename

    def __call__(self, val_loss, model):
        score = -val_loss
        if self.best_score is None:
            self.best_score = score
            self.save_checkpoint(val_loss, model)
        elif score < self.best_score:
            self.counter += 1
            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')
            if self.counter >= self.patience:
                self.early_stop = True
        else:
            self.best_score = score
            self.save_checkpoint(val_loss, model)
            self.counter = 0

    def save_checkpoint(self, val_loss, model):
        if self.verbose: print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')
        torch.save(model.state_dict(),basepath+'/CheckPoints/'+self.filename+'.pt')
        self.val_loss_min = val_loss

def get_data_loaders():
  transform = transforms.Compose([transforms.Resize((32,32)),transforms.ToTensor(),transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])])
  cifar10_train = torchvision.datasets.CIFAR10('../data',transform=transform,train=True,download=True)
  cifar10_test = torchvision.datasets.CIFAR10('../data',transform=transform,train=False,download=True)
  data_loader_train = torch.utils.data.DataLoader(cifar10_train, batch_size=1024, shuffle=True, num_workers=8)
  data_loader_test = torch.utils.data.DataLoader(cifar10_test, batch_size=2048, shuffle=True, num_workers=8)
  return(data_loader_train,data_loader_test)

data_loader_train,data_loader_test = get_data_loaders()

def defineMasks(model):
  """
  Defines masks of ones for each prunable layer (Conv2d and Linear)
  in the given model, matching the shape of the layer's weight tensor.
  """
  conv_masks = []
  linear_masks = []

  # Iterate through the model's layers to get weight shapes
  for name, module in model.named_modules():
      if isinstance(module, nn.Conv2d):
          # Create a mask of ones with the same shape as the layer's weight
          mask = torch.ones_like(module.weight.data)
          conv_masks.append(mask)
          # print(f"Defined mask for Conv2d layer: {name} with shape {mask.shape}") # Optional: for debugging

      elif isinstance(module, nn.Linear):
          # Create a mask of ones with the same shape as the layer's weight
          mask = torch.ones_like(module.weight.data)
          linear_masks.append(mask)
          # print(f"Defined mask for Linear layer: {name} with shape {mask.shape}") # Optional: for debugging

  return conv_masks, linear_masks

import torch
import torch.nn as nn
import torch.nn.functional as F

class BasicBlock(nn.Module):
    def __init__(self, in_channels, out_channels, stride=1):
        super().__init__()
        self.conv1 = nn.Conv2d(in_channels, out_channels, 3, stride, 1, bias=False)
        self.bn1 = nn.BatchNorm2d(out_channels)
        self.conv2 = nn.Conv2d(out_channels, out_channels, 3, 1, 1, bias=False)
        self.bn2 = nn.BatchNorm2d(out_channels)

        self.downsample = None
        if stride != 1 or in_channels != out_channels:
            self.downsample = nn.Sequential(
                nn.Conv2d(in_channels, out_channels, 1, stride, bias=False),
                nn.BatchNorm2d(out_channels)
            )

    def forward(self, x):
        identity = x
        out = F.relu(self.bn1(self.conv1(x)))
        out = self.bn2(self.conv2(out))
        if self.downsample:
            identity = self.downsample(x)
        return F.relu(out + identity)

class ResNet18_CIFAR10(nn.Module):
    def __init__(self, num_classes=10):
        super().__init__()

        # Feature extractor
        self.features = nn.Sequential(
            nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            self._make_layer(64, 64, 2, stride=1),
            self._make_layer(64, 128, 2, stride=2),
            self._make_layer(128, 256, 2, stride=2),
            self._make_layer(256, 512, 2, stride=2),
            nn.AdaptiveAvgPool2d((1, 1))
        )

        # Classifier head
        self.classifier = nn.Linear(512, num_classes)

    def _make_layer(self, in_ch, out_ch, blocks, stride):
        layers = [BasicBlock(in_ch, out_ch, stride)]
        for _ in range(1, blocks):
            layers.append(BasicBlock(out_ch, out_ch))
        return nn.Sequential(*layers)

    def forward(self, x):
        x = self.features(x)              # (B, 512, 1, 1)
        x = torch.flatten(x, 1)           # (B, 512)
        return self.classifier(x)         # (B, 10)

import torch
import torch.nn as nn
import torch.nn.functional as F

class BasicBlock(nn.Module):
    def __init__(self, in_channels, out_channels, stride=1):
        super().__init__()
        self.conv1 = nn.Conv2d(in_channels, out_channels, 3, stride, 1, bias=False)
        self.bn1 = nn.BatchNorm2d(out_channels)
        self.conv2 = nn.Conv2d(out_channels, out_channels, 3, 1, 1, bias=False)
        self.bn2 = nn.BatchNorm2d(out_channels)

        self.downsample = None
        if stride != 1 or in_channels != out_channels:
            self.downsample = nn.Sequential(
                nn.Conv2d(in_channels, out_channels, 1, stride, bias=False),
                nn.BatchNorm2d(out_channels)
            )

    def forward(self, x):
        identity = x
        out = F.relu(self.bn1(self.conv1(x)))
        out = self.bn2(self.conv2(out))
        if self.downsample:
            identity = self.downsample(x)
        return F.relu(out + identity)

class FeaturesWithSkip(nn.Module):
    def __init__(self):
        super().__init__()

        self.conv = nn.Sequential(
            nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True)
        )

        self.layer1 = self._make_layer(64, 64, 2, stride=1)
        self.layer2 = self._make_layer(64, 128, 2, stride=2)
        self.layer3 = self._make_layer(128, 256, 2, stride=2)
        self.layer4 = self._make_layer(256, 512, 2, stride=2)
        self.pool = nn.AdaptiveAvgPool2d((1, 1))

        # CNN Skip connection: 64 â†’ 256, downsample spatial and increase channels
        self.skip_64_to_256 = nn.Sequential(
            nn.Conv2d(64, 256, kernel_size=1, stride=4, bias=False),
            nn.BatchNorm2d(256)
        )

    def _make_layer(self, in_ch, out_ch, blocks, stride):
        layers = [BasicBlock(in_ch, out_ch, stride)]
        for _ in range(1, blocks):
            layers.append(BasicBlock(out_ch, out_ch))
        return nn.Sequential(*layers)

    def forward(self, x):
        x = self.conv(x)
        x1 = self.layer1(x)      # 64 channels
        x2 = self.layer2(x1)     # 128 channels
        x3 = self.layer3(x2)     # 256 channels

        skip = self.skip_64_to_256(x1)  # Adjust dimensions to match x3
        x3 = x3 + skip

        x4 = self.layer4(x3)     # 512 channels
        out = self.pool(x4)      # (B, 512, 1, 1)
        return out               # Shape: (B, 512, 1, 1)

class ResNet18_CIFAR10(nn.Module):
    def __init__(self, num_classes=10):
        super().__init__()
        self.features = FeaturesWithSkip()
        self.classifier = nn.Linear(512, num_classes)

    def forward(self, x):
        x = self.features(x)        # (B, 512, 1, 1)
        x = torch.flatten(x, 1)     # (B, 512)
        return self.classifier(x)   # (B, 10)

num_classes = 10
device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')

mask_conv1, mask_conv2, mask_conv3, mask_conv4, mask_conv5, mask_conv6, mask_conv7, mask_conv8, mask_conv9, mask_conv10, mask_conv11, mask_conv12, mask_conv13, mask_linear1, mask_linear2, mask_linear3 = defineMasks()

def get_unpruned_model(num_classes):
  model = ResNet18_CIFAR10()
  return(model)

model = get_unpruned_model(num_classes)

def plot_losses(train_loss,test_loss,filename,i):
  x_axis = list(range(1,len(train_loss)+1))
  fig = plt.figure(i,figsize=(20,10))
  plt.axes(yscale='log')
  plt.plot(x_axis, train_loss, color='blue', label='Train Loss Loss')
  plt.plot(x_axis, test_loss, color='green', label='Validation Loss')
  plt.legend(loc=1)
  plt.xlabel('Iterations over entire dataset')
  plt.ylabel('Loss')
  plt.ylim(0.01,3)
  plt.savefig(basepath+'/Losses/'+filename+'.png')
  plt.close(fig)

def plot_accuracies(train_accuracy,test_accuracy,filename,i):
  x_axis = list(range(1,len(train_accuracy)+1))
  fig = plt.figure(i,figsize=(20,10))
  plt.yscale('log')
  plt.plot(x_axis, train_accuracy, color='blue', label='Train Accuracy')
  plt.plot(x_axis, test_accuracy, color='green', label='Validation Accuracy')
  plt.legend(loc=1)
  plt.xlabel('Iterations over entire dataset')
  plt.ylabel('Accuracy')
  plt.ylim(0.1,1)
  plt.savefig(basepath+'/Accuracies/'+filename+'.png')
  plt.close(fig)

import os
def train(model, prune_itr, lr_multiplier=1):
    scale_factor = 16

    learning_rate = 0.001 * lr_multiplier
    weight_decay = 5e-4
    lr_patience = int(2000 / scale_factor)
    lr_stepsize = int(3000 / scale_factor)
    es_patience = int(5000 / scale_factor)

    criterion = nn.CrossEntropyLoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)
    scheduler_plateau = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=lr_patience, threshold=1e-7, verbose=True)
    scheduler_stepLR = StepLR(optimizer, step_size=lr_stepsize, gamma=0.8)

    early_stopping = EarlyStopping(patience=es_patience, filename='checkpoint_' + str(prune_itr), verbose=True)
    os.makedirs(os.path.join(basepath, 'CheckPoints'), exist_ok=True)

    model.to(device)
    torch.cuda.empty_cache()

    num_epochs = 50
    best_loss = float('inf')
    best_accuracy = 0.0
    es_flag = False

    train_loss, test_loss = [], []
    train_accuracy, test_accuracy = [], []

    for epoch in range(num_epochs):
        model.train()
        correct = 0
        data_size = 0
        epoch_loss = 0

        for images, labels in data_loader_train:
            images, labels = images.to(device), labels.to(device)

            optimizer.zero_grad()
            outputs = model(images)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()

            _, predicted = torch.max(outputs, 1)
            correct += (predicted == labels).sum().item()
            data_size += labels.size(0)
            epoch_loss += loss.item()

        train_acc = correct / data_size
        train_loss_epoch = epoch_loss / len(data_loader_train)
        train_loss.append(train_loss_epoch)
        train_accuracy.append(train_acc)

        # Validation
        model.eval()
        correct_t, data_size_t, val_loss = 0, 0, 0
        with torch.no_grad():
            for images, labels in data_loader_test:
                images, labels = images.to(device), labels.to(device)
                outputs = model(images)
                loss = criterion(outputs, labels)
                val_loss += loss.item()
                _, predicted = torch.max(outputs, 1)
                correct_t += (predicted == labels).sum().item()
                data_size_t += labels.size(0)

        val_loss_epoch = val_loss / len(data_loader_test)
        val_acc = correct_t / data_size_t
        test_loss.append(val_loss_epoch)
        test_accuracy.append(val_acc)

        print(f"Epoch {epoch+1}/{num_epochs} | "
              f"Train Loss: {train_loss_epoch:.4f} | Train Acc: {train_acc:.2%} | "
              f"Val Loss: {val_loss_epoch:.4f} | Val Acc: {val_acc:.2%}")

        if val_loss_epoch < best_loss:
            best_loss = val_loss_epoch
            best_accuracy = val_acc

        scheduler_stepLR.step()
        scheduler_plateau.step(val_loss_epoch)

        early_stopping(val_loss_epoch, model)
        if early_stopping.early_stop:
            print("Early stopping triggered.")
            es_flag = True
            break

    model.load_state_dict(torch.load(os.path.join(basepath, 'CheckPoints', 'checkpoint_' + str(prune_itr) + '.pt')))
    return model, train_loss, test_loss, train_accuracy, test_accuracy, best_loss, best_accuracy

model = ResNet18_CIFAR10()
x = torch.randn(1, 3, 32, 32)
print(model.features(x).shape)

lr_multiplier = 1
dict_best_info = {}
#4model.load_state_dict(torch.load(basepath+'/CheckPoints/'+'checkpoint_0.pt'))
model,train_loss,test_loss,train_accuracy,test_accuracy,best_loss,best_accuracy = train(model,0,lr_multiplier)
dict_best_info[0] = (best_loss,best_accuracy)
plot_losses(train_loss,test_loss,'l0',1)
plot_accuracies(train_accuracy,test_accuracy,'a0',2)

model.load_state_dict(torch.load(basepath+'/CheckPoints/'+'checkpoint_0.pt'))
model = model.to(device)
print(model)

print('Best Accuracy is ' + str(round(best_accuracy*100,2)))
print('Best Loss is ' + str(round(best_loss,3)))

for images, labels in data_loader_train:
    print(images.shape, labels)
    break

def get_model_layerwise_analysis(model):
    print("===== Feature Layers =====")
    for name, module in model.features.named_modules():
        if isinstance(module, torch.nn.Conv2d):
            weights = torch.abs(module.weight.data).cpu()
            print(f"{name:<20} | max: {weights.max():.4f}, mean: {weights.mean():.4f}, std: {weights.std():.4f}")

    print("\n===== Classifier Layers =====")
    for name, module in model.classifier.named_modules():
        if isinstance(module, torch.nn.Linear):
            weights = torch.abs(module.weight.data).cpu()
            print(f"{name:<20} | max: {weights.max():.4f}, mean: {weights.mean():.4f}, std: {weights.std():.4f}")
get_model_layerwise_analysis(model)

initial_parameters = sum([p.numel() for p in model.parameters()])
initial_parameters

import torch.nn as nn

def update_masks(model, conv_masks, linear_masks, ks, device):
    # Move masks to device
    conv_masks = [m.to(device) for m in conv_masks]
    linear_masks = [m.to(device) for m in linear_masks]

    updated_conv_masks = []
    conv_idx = 0

    # Iterate through feature layers (Conv2d)
    for _, module in model.features.named_children():
        if isinstance(module, nn.Conv2d):
            weight = torch.abs(module.weight)
            k = ks[conv_idx]

            # Use formula for a and b
            a = 0.25 * weight.max().data * k
            t = 2.0 * weight.std().data
            b = a + t

            # Update mask
            temp_mask1 = (weight > b).float()
            temp_mask2 = (weight > a).float()
            temp_mask = (temp_mask2 * conv_masks[conv_idx]) + temp_mask1
            updated_mask = (temp_mask >= 1.0).float()

            updated_conv_masks.append(updated_mask)
            conv_idx += 1

    updated_linear_masks = []
    linear_idx = 0

    # Iterate through classifier layers (Linear)
    for _, module in model.classifier.named_children():
        if isinstance(module, nn.Linear):
            weight = torch.abs(module.weight)
            k = ks[conv_idx + linear_idx]  # continue index from conv layers

            if linear_idx == 0:
                a = 0.35 * weight.max().data * k
                t = 2.0 * weight.std().data
            elif linear_idx == 1:
                a = 0.25 * weight.max().data * k
                t = 1.0 * weight.std().data
            else:
                a = 0.20 * weight.max().data * k
                t = 1.0 * weight.std().data
            b = a + t

            # Update mask
            temp_mask1 = (weight > b).float()
            temp_mask2 = (weight > a).float()
            temp_mask = (temp_mask2 * linear_masks[linear_idx]) + temp_mask1
            updated_mask = (temp_mask >= 1.0).float()

            updated_linear_masks.append(updated_mask)
            linear_idx += 1

    # Return updated masks on GPU
    return updated_conv_masks, updated_linear_masks

def model_surgery(model, mask_conv1, mask_conv2, mask_conv3, mask_conv4, mask_conv5, mask_conv6, mask_conv7, mask_conv8, mask_conv9, mask_conv10, mask_conv11, mask_conv12, mask_conv13, mask_linear1, mask_linear2, mask_linear3):

  for layer, (name, module) in enumerate(model.features._modules.items()):
    if name == '0':
      weights = (module.weight.data) * mask_conv1.to(device)
      model.features._modules[name].weight.data = weights.to(device)
    if name == '2':
      weights = (module.weight.data) * mask_conv2.to(device)
      model.features._modules[name].weight.data = weights.to(device)
    if name == '5':
      weights = (module.weight.data) * mask_conv3.to(device)
      model.features._modules[name].weight.data = weights.to(device)
    if name == '7':
      weights = (module.weight.data) * mask_conv4.to(device)
      model.features._modules[name].weight.data = weights.to(device)
    if name == '10':
      weights = (module.weight.data) * mask_conv5.to(device)
      model.features._modules[name].weight.data = weights.to(device)
    if name == '12':
      weights = (module.weight.data) * mask_conv6.to(device)
      model.features._modules[name].weight.data = weights.to(device)
    if name == '14':
      weights = (module.weight.data) * mask_conv7.to(device)
      model.features._modules[name].weight.data = weights.to(device)
    if name == '17':
      weights = (module.weight.data) * mask_conv8.to(device)
      model.features._modules[name].weight.data = weights.to(device)
    if name == '19':
      weights = (module.weight.data) * mask_conv9.to(device)
      model.features._modules[name].weight.data = weights.to(device)
    if name == '21':
      weights = (module.weight.data) * mask_conv10.to(device)
      model.features._modules[name].weight.data = weights.to(device)
    if name == '24':
      weights = (module.weight.data) * mask_conv11.to(device)
      model.features._modules[name].weight.data = weights.to(device)
    if name == '26':
      weights = (module.weight.data) * mask_conv12.to(device)
      model.features._modules[name].weight.data = weights.to(device)
    if name == '28':
      weights = (module.weight.data) * mask_conv13.to(device)
      model.features._modules[name].weight.data = weights.to(device)

  for layer, (name, module) in enumerate(model.classifier._modules.items()):
    if name == '0':
      weights = (module.weight.data) * mask_linear1.to(device)
      model.classifier._modules[name].weight.data = weights.to(device)
    if name == '3':
      weights = (module.weight.data) * mask_linear2.to(device)
      model.classifier._modules[name].weight.data = weights.to(device)
    if name == '6':
      weights = (module.weight.data) * mask_linear3.to(device)
      model.classifier._modules[name].weight.data = weights.to(device)

  for param in model.parameters(): param.requires_grad = True

  return(model)

import torch.nn as nn

def model_surgery(model, conv_masks, linear_masks):
    # Apply conv masks
    conv_idx = 0
    for module in model.features:
        if isinstance(module, nn.Conv2d):
            mask = conv_masks[conv_idx]
            module.weight.data.mul_(mask)  # In-place masking
            conv_idx += 1

    # Apply linear masks
    linear_idx = 0
    for module in model.classifier:
        if isinstance(module, nn.Linear):
            mask = linear_masks[linear_idx]
            module.weight.data.mul_(mask)  # In-place masking
            linear_idx += 1

    # Ensure all params are trainable
    for param in model.parameters():
        param.requires_grad = True

    return model

def perform_surgery_training(model, prune_itr, lr_multiplier, conv_masks, linear_masks):
    scale_factor = 16

    learning_rate = 1e-3 * lr_multiplier
    wt_dcy = 1e-7
    lr_patience = int(2000 / scale_factor)
    lr_stepsize = 1
    es_patience = int(10000 / scale_factor)

    criterion = nn.CrossEntropyLoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=wt_dcy)
    scheduler_plateau = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=lr_patience, verbose=True, threshold=1e-7)
    scheduler_stepLR = StepLR(optimizer, step_size=lr_stepsize, gamma=0.995)
    early_stopping = EarlyStopping(patience=es_patience, filename='checkpoint_' + str(prune_itr), verbose=True)

    torch.cuda.empty_cache()
    model = model.to(device)
    num_epochs = 70
    es_flag = 0

    final_model = deepcopy(model)

    train_loss, test_loss = [], []
    train_accuracy, test_accuracy = [], []
    nonzero_parameter_list = []

    best_loss = 1e5
    best_accuracy = 0
    best_parameter_size = 0

    prob_itr = 0
    prob_threshold = 1.0

    # Initialization constants (k1...k16)
    ks = [1.00015, 1.00014, 1.0001, 1.0001, 1.0001, 1.00007,
          1.00012, 1.0009, 1.0009, 1.0009, 1.0009, 1.0009,
          1.0009, 1.0009, 1.0009, 1.0009]

    for epoch in range(num_epochs):
        correct = 0
        data_size = 0
        t_loss = 0

        for i, (images, labels) in enumerate(data_loader_train):
            model.train()
            images, labels = images.to(device), labels.to(device)
            data_size += len(images)

            outputs = torch.nn.functional.log_softmax(model(images), dim=1)
            trainloss = criterion(outputs, labels)

            optimizer.zero_grad()
            trainloss.backward()

            choice = random.random()
            if choice <= prob_threshold:
                print(f'Performing Surgery: choice = {round(choice, 4)} <= threshold = {round(prob_threshold, 4)}')
                conv_masks, linear_masks = update_masks(model, conv_masks, linear_masks, ks,device)
                ks = [k * 1.00005 for k in ks]

            optimizer.step()
            model = model_surgery(model, conv_masks, linear_masks)

            prob_itr += 1
            prob_threshold *= 0.9993

            nonzero_parameters = count_nonzeros(model)
            nonzero_parameter_list.append(nonzero_parameters)
            print(f'Parameters are: {nonzero_parameters}')

            _, predicted = torch.max(outputs.data, 1)
            correct += (predicted == labels).sum().item()
            t_loss += trainloss.item()

            # Validation
            model.eval()
            correct_t = 0
            data_size_t = 0
            tt_loss = 0
            with torch.no_grad():
                for images, labels in data_loader_test:
                    images, labels = images.to(device), labels.to(device)
                    data_size_t += len(images)
                    outputs = torch.nn.functional.log_softmax(model(images), dim=1)
                    _, predicted = torch.max(outputs.data, 1)
                    correct_t += (predicted == labels).sum().item()
                    tt_loss += criterion(outputs, labels).item()

            acc_train = 100 * correct / data_size
            acc_val = 100 * correct_t / data_size_t
            loss_val = tt_loss / len(data_loader_test)
            print(f'Epoch: {epoch+1}, Batch: {i+1}, Train Loss: {trainloss.item():.3f}, Train Acc: {acc_train:.2f}%, Val Loss: {loss_val:.3f}, Val Acc: {acc_val:.2f}%')

            train_loss.append(trainloss.item())
            test_loss.append(loss_val)
            train_accuracy.append(acc_train / 100)
            test_accuracy.append(acc_val / 100)

            if loss_val < best_loss:
                best_loss = loss_val
                best_accuracy = correct_t / data_size_t
                best_parameter_size = nonzero_parameters

            scheduler_stepLR.step()
            scheduler_plateau.step(loss_val)

            early_stopping(loss_val, model)
            if early_stopping.early_stop:
                print("Early stopping")
                es_flag = 1
                break

        if es_flag == 1:
            break

    if acc_val >= (best_accuracy * 0.9):
        last_loss = loss_val
        last_accuracy = correct_t / data_size_t
        last_parameter_size = nonzero_parameters
        final_model = deepcopy(model)
    else:
        last_loss, last_accuracy, last_parameter_size = None, None, None

    model.load_state_dict(torch.load(basepath + '/CheckPoints/' + f'checkpoint_{prune_itr}.pt'))

    return (
        model, final_model,
        train_loss, test_loss,
        train_accuracy, test_accuracy,
        best_loss, best_accuracy, best_parameter_size,
        last_loss, last_accuracy, last_parameter_size,
        nonzero_parameter_list
    )

def plot_parameters(parameter_list,filename,i):
  x_axis = list(range(1,len(parameter_list)+1))
  fig = plt.figure(i,figsize=(20,10))
  plt.plot(x_axis, parameter_list, color='blue', label='Total Parameters')
  plt.legend(loc=1)
  plt.xlabel('Iterations over entire dataset')
  plt.ylabel('Parameters')
  plt.ylim(0,70000)
  plt.savefig(basepath+'/Parameters/'+filename+'.png')
  plt.close(fig)

import torch.nn as nn

def model_surgery(model, conv_masks, linear_masks):
    # Apply conv masks
    conv_idx = 0
    # Iterate through the modules within the features Sequential block
    for module in model.features:
        # Check if the current module is a Conv2d layer
        if isinstance(module, nn.Conv2d):
            # Apply the corresponding mask from the conv_masks list
            mask = conv_masks[conv_idx]
            module.weight.data.mul_(mask)  # In-place masking of weights
            conv_idx += 1
        # If the module is a Sequential block (like the BasicBlocks), iterate through its children
        elif isinstance(module, nn.Sequential):
            for sub_module in module.children():
                # Iterate through the modules within the BasicBlock
                for layer_name, layer in sub_module.named_children():
                    # Check for Conv2d layers within the BasicBlock
                    if isinstance(layer, nn.Conv2d):
                        mask = conv_masks[conv_idx]
                        layer.weight.data.mul_(mask) # In-place masking of weights
                        conv_idx += 1


    # Apply linear masks
    # Since the classifier is a single Linear layer, access it directly
    if isinstance(model.classifier, nn.Linear):
        mask = linear_masks[0] # Access the first (and only) linear mask
        model.classifier.weight.data.mul_(mask) # In-place masking of weights
    elif isinstance(model.classifier, nn.Sequential):
        linear_idx = 0
        # Iterate through the modules within the classifier Sequential block
        for module in model.classifier:
            if isinstance(module, nn.Linear):
                mask = linear_masks[linear_idx]
                module.weight.data.mul_(mask)  # In-place masking
                linear_idx += 1


    # Ensure all parameters are trainable
    for param in model.parameters():
        param.requires_grad = True

    return model

import torch.nn as nn

def update_masks(model, conv_masks, linear_masks, ks, device):
    # Move masks to device
    conv_masks = [m.to(device) for m in conv_masks]
    linear_masks = [m.to(device) for m in linear_masks]

    updated_conv_masks = []
    conv_idx = 0

    # Iterate through feature layers (Conv2d)
    # Need to traverse through the Sequential and BasicBlock structure
    for name, module in model.features.named_children():
        if isinstance(module, nn.Conv2d):
            # This handles the initial Conv2d layer
            weight = torch.abs(module.weight)
            k = ks[conv_idx]

            # Use formula for a and b
            a = 0.25 * weight.max().data * k
            t = 2.0 * weight.std().data
            b = a + t

            # Update mask
            temp_mask1 = (weight > b).float()
            temp_mask2 = (weight > a).float()
            temp_mask = (temp_mask2 * conv_masks[conv_idx]) + temp_mask1
            updated_mask = (temp_mask >= 1.0).float()

            updated_conv_masks.append(updated_mask)
            conv_idx += 1
        elif isinstance(module, nn.Sequential):
             # This handles the BasicBlock Sequential blocks
            for sub_module in module.children(): # Iterate through BasicBlocks
                if isinstance(sub_module, BasicBlock):
                    # Iterate through layers within the BasicBlock
                    for layer_name, layer in sub_module.named_children():
                        if isinstance(layer, nn.Conv2d):
                            weight = torch.abs(layer.weight)
                            # Ensure we don't go out of bounds of ks
                            if conv_idx < len(ks):
                                k = ks[conv_idx]
                            else:
                                print(f"Warning: Not enough k values for conv layer {conv_idx}")
                                k = 1.0 # Default k if not available

                            # Use formula for a and b (adjusting based on layer structure if needed, but keeping it simple for now)
                            a = 0.25 * weight.max().data * k
                            t = 2.0 * weight.std().data
                            b = a + t

                            # Update mask
                            temp_mask1 = (weight > b).float()
                            temp_mask2 = (weight > a).float()
                            temp_mask = (temp_mask2 * conv_masks[conv_idx]) + temp_mask1
                            updated_mask = (temp_mask >= 1.0).float()

                            updated_conv_masks.append(updated_mask)
                            conv_idx += 1
                        elif isinstance(layer, nn.Sequential): # Handle the downsample Sequential in BasicBlock
                             for ds_layer in layer.children():
                                 if isinstance(ds_layer, nn.Conv2d):
                                     weight = torch.abs(ds_layer.weight)
                                     if conv_idx < len(ks):
                                         k = ks[conv_idx]
                                     else:
                                         print(f"Warning: Not enough k values for conv layer {conv_idx}")
                                         k = 1.0 # Default k if not available

                                     a = 0.25 * weight.max().data * k
                                     t = 2.0 * weight.std().data
                                     b = a + t

                                     temp_mask1 = (weight > b).float()
                                     temp_mask2 = (weight > a).float()
                                     temp_mask = (temp_mask2 * conv_masks[conv_idx]) + temp_mask1
                                     updated_mask = (temp_mask >= 1.0).float()

                                     updated_conv_masks.append(updated_mask)
                                     conv_idx += 1


    updated_linear_masks = []
    linear_idx = 0

    # Iterate through classifier layers (Linear)
    # Access the single Linear layer directly
    if isinstance(model.classifier, nn.Linear):
        weight = torch.abs(model.classifier.weight)
        # Ensure we have a k for the linear layer. It seems ks has 16 values.
        # If you only have one linear layer mask (linear_masks has 1 item),
        # you might need to adjust your ks or how you index it.
        # Let's assume k14 is for the first linear layer based on your original logic.
        # You'll need to map ks indices correctly to your linear layers.
        # Assuming ks are ordered for convs then linears
        linear_k_index = conv_idx + linear_idx # Continue indexing from the last conv layer
        if linear_k_index < len(ks):
             k = ks[linear_k_index]
        else:
             print(f"Warning: Not enough k values for linear layer {linear_idx}")
             k = 1.0 # Default k if not available

        # Using the formula for the first linear layer from your original code
        a = 0.35 * weight.max().data * k
        t = 2.0 * weight.std().data
        b = a + t

        # Update mask
        # Access the first (and only) linear mask
        temp_mask1 = (weight > b).float()
        temp_mask2 = (weight > a).float()
        temp_mask = (temp_mask2 * linear_masks[0]) + temp_mask1 # Use index 0 as it's a list with one mask
        updated_mask = (temp_mask >= 1.0).float()

        updated_linear_masks.append(updated_mask)
        linear_idx += 1 # Increment linear_idx even if there's only one layer for clarity
    elif isinstance(model.classifier, nn.Sequential):
         linear_idx = 0
         for module in model.classifier.children():
              if isinstance(module, nn.Linear):
                  weight = torch.abs(module.weight)
                  linear_k_index = conv_idx + linear_idx
                  if linear_k_index < len(ks):
                      k = ks[linear_k_index]
                  else:
                      print(f"Warning: Not enough k values for linear layer {linear_idx}")
                      k = 1.0

                  # Using the different formulas for linear layers
                  if linear_idx == 0:
                       a = 0.35 * weight.max().data * k
                       t = 2.0 * weight.std().data
                  elif linear_idx == 1:
                       a = 0.25 * weight.max().data * k
                       t = 1.0 * weight.std().data
                  else: # Assuming the third linear layer
                       a = 0.20 * weight.max().data * k
                       t = 1.0 * weight.std().data
                  b = a + t

                  temp_mask1 = (weight > b).float()
                  temp_mask2 = (weight > a).float()
                  temp_mask = (temp_mask2 * linear_masks[linear_idx]) + temp_mask1
                  updated_mask = (temp_mask >= 1.0).float()

                  updated_linear_masks.append(updated_mask)
                  linear_idx += 1


    # Return updated masks on GPU
    return updated_conv_masks, updated_linear_masks

import torch.nn as nn

def perform_surgery_training(model, prune_itr, lr_multiplier, conv_masks, linear_masks):
    scale_factor = 16

    learning_rate = 1e-3 * lr_multiplier
    wt_dcy = 1e-7
    lr_patience = int(2000 / scale_factor)
    lr_stepsize = 1
    es_patience = int(10000 / scale_factor)

    criterion = nn.CrossEntropyLoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=wt_dcy)
    scheduler_plateau = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=lr_patience, verbose=True, threshold=1e-7)
    scheduler_stepLR = StepLR(optimizer, step_size=lr_stepsize, gamma=0.995)
    early_stopping = EarlyStopping(patience=es_patience, filename='checkpoint_' + str(prune_itr), verbose=True)

    torch.cuda.empty_cache()
    model = model.to(device)
    num_epochs = 70
    es_flag = 0

    final_model = deepcopy(model)

    train_loss, test_loss = [], []
    train_accuracy, test_accuracy = [], []
    nonzero_parameter_list = []

    best_loss = 1e5
    best_accuracy = 0
    best_parameter_size = 0

    prob_itr = 0
    prob_threshold = 1.0

    # Initialization constants (k1...k16)
    ks = [1.00015, 1.00014, 1.0001, 1.0001, 1.0001, 1.00007,
          1.00012, 1.0009, 1.0009, 1.0009, 1.0009, 1.0009,
          1.0009, 1.0009, 1.0009, 1.0009, 1.0009, 1.0009, 1.0009, 1.0009, 1.0009,10009] # Ensure ks has enough values for all prunable layers

    for epoch in range(num_epochs):
        correct = 0
        data_size = 0
        t_loss = 0

        for i, (images, labels) in enumerate(data_loader_train):
            model.train()
            images, labels = images.to(device), labels.to(device)
            data_size += len(images)

            # Using log_softmax might not be ideal with CrossEntropyLoss,
            # which expects raw logits. Using CrossEntropyLoss directly is usually better.
            # outputs = torch.nn.functional.log_softmax(model(images), dim=1)
            outputs = model(images)
            trainloss = criterion(outputs, labels)

            optimizer.zero_grad()
            trainloss.backward()

            choice = random.random()
            if choice <= prob_threshold:
                print(f'Performing Surgery: choice = {round(choice, 4)} <= threshold = {round(prob_threshold, 4)}')
                # Pass the lists of masks and the ks list
                conv_masks, linear_masks = update_masks(model, conv_masks, linear_masks, ks, device)
                # Update ks - consider how you want to increment these values.
                # Multiplying all by a constant factor might be too aggressive or too slow.
                # ks = [k * 1.00005 for k in ks] # Example of uniform increment

            optimizer.step()
            # Pass the model and the updated masks
            model = model_surgery(model, conv_masks, linear_masks)

            prob_itr += 1
            # Decrease the probability threshold over time
            prob_threshold *= 0.9993

            # Ensure count_nonzeros function is defined elsewhere if you use it
            nonzero_parameters = count_nonzeros(model)
            nonzero_parameter_list.append(nonzero_parameters)
            print(f'Parameters are: {nonzero_parameters}')

            _, predicted = torch.max(outputs.data, 1)
            # The correct count should be over the entire epoch, not per batch
            # correct += (predicted == labels.data).sum().item()
            # t_loss += trainloss.item()

            # It seems you're running validation and printing stats per batch.
            # It's more standard to do this per epoch.
            # For now, keeping the per-batch validation logic but note it's unusual.

            # Validation
            model.eval()
            correct_t = 0
            data_size_t = 0
            tt_loss = 0
            with torch.no_grad():
                for val_images, val_labels in data_loader_test:
                    val_images, val_labels = val_images.to(device), val_labels.to(device)
                    data_size_t += len(val_images)
                    # outputs_val = torch.nn.functional.log_softmax(model(val_images), dim=1)
                    outputs_val = model(val_images)
                    _, predicted_val = torch.max(outputs_val.data, 1)
                    correct_t += (predicted_val == val_labels).sum().item()
                    tt_loss += criterion(outputs_val, val_labels).item()

            acc_train_batch = 100 * (predicted == labels).sum().item() / len(images) # Accuracy for the current training batch
            acc_val_epoch = 100 * correct_t / data_size_t # Accuracy for the entire validation dataset
            loss_val_epoch = tt_loss / len(data_loader_test) # Average loss for the entire validation dataset

            print(f'Epoch: {epoch+1}, Batch: {i+1}, Train Loss: {trainloss.item():.3f}, Train Acc (Batch): {acc_train_batch:.2f}%, Val Loss (Epoch): {loss_val_epoch:.3f}, Val Acc (Epoch): {acc_val_epoch:.2f}%')

            # It's more standard to record epoch-level metrics
            # train_loss.append(trainloss_epoch / len(data_loader_train))
            # test_loss.append(loss_val_epoch)
            # train_accuracy.append(correct / data_size)
            # test_accuracy.append(correct_t / data_size_t)

            # If you must record per-batch, adjust your plotting and analysis accordingly.
            train_loss.append(trainloss.item())
            test_loss.append(loss_val_epoch) # Still using epoch validation loss per batch
            train_accuracy.append(acc_train_batch / 100)
            test_accuracy.append(acc_val_epoch / 100) # Still using epoch validation accuracy per batch


            # Use epoch validation loss for early stopping and scheduler
            if loss_val_epoch < best_loss:
                best_loss = loss_val_epoch
                best_accuracy = correct_t / data_size_t
                best_parameter_size = nonzero_parameters

            # Step the scheduler based on epoch-level validation loss
            # Consider if stepping scheduler per batch is intended. Usually per epoch.
            scheduler_stepLR.step()
            scheduler_plateau.step(loss_val_epoch)

            # Early stopping uses epoch validation loss
            early_stopping(loss_val_epoch, model)
            if early_stopping.early_stop:
                print("Early stopping")
                es_flag = 1
                break
        if es_flag == 1:
            break

    # Calculate final metrics after training finishes or stops early
    # Need to run evaluation one last time on the final model state
    model.eval()
    correct_final = 0
    data_size_final = 0
    loss_final = 0
    with torch.no_grad():
        for images, labels in data_loader_test:
            images, labels = images.to(device), labels.to(device)
            data_size_final += len(images)
            outputs_final = model(images)
            _, predicted_final = torch.max(outputs_final.data, 1)
            correct_final += (predicted_final == labels).sum().item()
            loss_final += criterion(outputs_final, labels).item()

    last_loss = loss_final / len(data_loader_test)
    last_accuracy = correct_final / data_size_final
    last_parameter_size = count_nonzeros(model) # Recalculate parameter count on the final model state

    # Check if the final accuracy is within 90% of the best accuracy seen
    # This check seems a bit unusual for the final model state - it might be better to
    # just return the best model saved by EarlyStopping.
    # If the goal is to use the LAST model state *if* its performance is close to best,
    # then keep this logic. Otherwise, the final_model deepcopy might not be necessary
    # and you'd just return the model loaded from the best checkpoint.
    if last_accuracy >= (best_accuracy * 0.9):
         # final_model is already the last state due to deepcopy at start and modifications
         pass
    else:
        # Load the state dict from the best checkpoint into final_model
        final_model.load_state_dict(torch.load(os.path.join(basepath, 'CheckPoints', f'checkpoint_{prune_itr}.pt')))
        # Update last_loss, last_accuracy, last_parameter_size to reflect the loaded best model's stats
        # You might need to re-evaluate the loaded best model to get these exact values
        # Or perhaps you stored them when saving the best checkpoint.
        # For now, setting them to None or the best known values
        last_loss = best_loss
        last_accuracy = best_accuracy
        # Recalculate parameter count for the best model if needed, or store it
        # last_parameter_size = count_nonzeros(final_model)


    # Load the model from the best checkpoint before returning
    model.load_state_dict(torch.load(os.path.join(basepath, 'CheckPoints', f'checkpoint_{prune_itr}.pt')))


    return (
        model, final_model,
        train_loss, test_loss,
        train_accuracy, test_accuracy,
        best_loss, best_accuracy, best_parameter_size,
        last_loss, last_accuracy, last_parameter_size,
        nonzero_parameter_list
    )

# Need to ensure `count_nonzeros` is defined.
def count_nonzeros(model):
    """Counts the number of non-zero parameters in the model."""
    nonzero_count = 0
    for param in model.parameters():
        nonzero_count += torch.count_nonzero(param).item()
    return nonzero_count

new_model = deepcopy(model)
conv_masks, linear_masks = defineMasks(new_model)
lr_multiplier = 1
#new_model.load_state_dict(torch.load(basepath+'/CheckPoints/'+'checkpoint_1.pt'))
pruned_model, final_model, train_loss, test_loss, train_accuracy, test_accuracy, best_loss, best_accuracy, best_parameter_size, last_loss, last_accuracy, last_parameter_size, parameter_list = perform_surgery_training(new_model, 1, lr_multiplier, conv_masks, linear_masks)
dict_best_info[1] = (best_loss,best_accuracy)
plot_losses(train_loss,test_loss,'l1',1)
plot_accuracies(train_accuracy,test_accuracy,'a1',2)
plot_parameters(parameter_list,'p1',3)

pruned_model.load_state_dict(torch.load(basepath+'/CheckPoints/'+'checkpoint_1.pt'))
pruned_model = pruned_model.to(device)
print(pruned_model)

pruned_model = deepcopy(model)
pruned_model.load_state_dict(torch.load(basepath+'/CheckPoints/'+'checkpoint_1.pt',map_location=lambda storage, loc: storage))
pruned_model = pruned_model.to(device)
print(pruned_model)

import torch.nn as nn
import torch

def defineMasks(model):
  """
  Defines masks of ones for each prunable layer (Conv2d and Linear)
  in the given model, matching the shape of the layer's weight tensor.
  """
  conv_masks = []
  linear_masks = []

  # Iterate through the model's layers using named_modules for consistent ordering
  for name, module in model.named_modules():
      if isinstance(module, nn.Conv2d):
          # Create a mask of ones with the same shape as the layer's weight
          mask = torch.ones_like(module.weight.data)
          conv_masks.append(mask)

      elif isinstance(module, nn.Linear):
          # Create a mask of ones with the same shape as the layer's weight
          mask = torch.ones_like(module.weight.data)
          linear_masks.append(mask)

  return conv_masks, linear_masks

import torch.nn as nn
import torch.nn.functional as F

# Assuming BasicBlock class is defined elsewhere and accessible

def update_masks(model, conv_masks, linear_masks, ks, device):
    # Move masks to device
    # Note: It's generally better to perform operations on the same device
    # as the model weights to avoid unnecessary transfers. Let's modify
    # the mask creation in defineMasks to create masks on the correct device initially.
    # Or, ensure masks are moved to device before any operations involving model weights.
    # For now, we'll keep moving them here but recommend creating them on device.

    updated_conv_masks = []
    conv_idx = 0

    updated_linear_masks = []
    linear_idx = 0

    # Iterate through the model's layers using named_modules for consistent ordering
    for name, module in model.named_modules():
        if isinstance(module, nn.Conv2d):
            # Ensure we have enough masks and k values
            if conv_idx >= len(conv_masks) or conv_idx >= len(ks):
                 print(f"Warning: Not enough masks or k values for Conv2d layer {name} (index {conv_idx}). Skipping mask update.")
                 # Append the original mask if available, or a new one if not
                 updated_conv_masks.append(conv_masks[conv_idx] if conv_idx < len(conv_masks) else torch.ones_like(module.weight.data).to(device))
                 conv_idx += 1
                 continue

            weight = torch.abs(module.weight) # Weight is already on device
            k = ks[conv_idx]
            current_mask = conv_masks[conv_idx].to(device) # Ensure mask is on device

            # Use formula for a and b - adjust formula based on specific layer properties if needed
            # For simplicity, using a general formula as in your original code
            # Note: The 'a' calculation uses max(), 't' uses std(). These might vary
            # depending on the layer (e.g., first conv vs subsequent convs, or downsamples).
            # Based on your original code, the multipliers for max() and std() were hardcoded
            # per layer type/position. To match that, you'd need a more complex mapping
            # or pass specific ks values per layer. Let's use a general approach for now,
            # assuming ks applies sequentially to all Conv2d layers.
            # If you need layer-specific a and t calculations, you'll need to enhance this logic.

            # Example general formula (simplified from your original hardcoded values)
            a = 0.25 * weight.max().data * k # Using 0.25 as a generic multiplier
            t = 2.0 * weight.std().data     # Using 2.0 as a generic multiplier
            b = a + t

            # Update mask
            temp_mask1 = (weight > b).float()
            temp_mask2 = (weight > a).float()
            # Use the mask from the input list for the update calculation
            temp_mask = (temp_mask2 * current_mask) + temp_mask1
            updated_mask = (temp_mask >= 1.0).float()

            updated_conv_masks.append(updated_mask)
            conv_idx += 1

        elif isinstance(module, nn.Linear):
             # Ensure we have enough masks and k values
            linear_k_index = len(conv_masks) + linear_idx # Continue indexing ks after convs
            if linear_idx >= len(linear_masks) or linear_k_index >= len(ks):
                 print(f"Warning: Not enough masks or k values for Linear layer {name} (index {linear_idx}). Skipping mask update.")
                 updated_linear_masks.append(linear_masks[linear_idx] if linear_idx < len(linear_masks) else torch.ones_like(module.weight.data).to(device))
                 linear_idx += 1
                 continue

            weight = torch.abs(module.weight) # Weight is already on device
            k = ks[linear_k_index]
            current_mask = linear_masks[linear_idx].to(device) # Ensure mask is on device

            # Use formula for a and b - apply the different formulas based on linear_idx
            if linear_idx == 0:
                 a = 0.35 * weight.max().data * k
                 t = 2.0 * weight.std().data
            elif linear_idx == 1:
                 a = 0.25 * weight.max().data * k
                 t = 1.0 * weight.std().data
            else: # Assuming the third linear layer (index 2)
                 a = 0.20 * weight.max().data * k
                 t = 1.0 * weight.std().data
            b = a + t


            # Update mask
            temp_mask1 = (weight > b).float()
            temp_mask2 = (weight > a).float()
            temp_mask = (temp_mask2 * current_mask) + temp_mask1
            updated_mask = (temp_mask >= 1.0).float()

            updated_linear_masks.append(updated_mask)
            linear_idx += 1

    # Return updated masks on GPU (as they were created/moved to device)
    return updated_conv_masks, updated_linear_masks


import torch.nn as nn

def model_surgery(model, conv_masks, linear_masks):
    conv_idx = 0
    linear_idx = 0

    # Iterate through the model's layers using named_modules for consistent ordering
    for name, module in model.named_modules():
        if isinstance(module, nn.Conv2d):
            # Ensure we have a mask for this layer
            if conv_idx < len(conv_masks):
                mask = conv_masks[conv_idx]
                # Ensure mask is on the same device as the module's weight
                module.weight.data.mul_(mask.to(module.weight.data.device))  # In-place masking
                conv_idx += 1
            else:
                 print(f"Warning: Not enough masks for Conv2d layer {name} (index {conv_idx}). Skipping masking.")


        elif isinstance(module, nn.Linear):
            # Ensure we have a mask for this layer
            if linear_idx < len(linear_masks):
                mask = linear_masks[linear_idx]
                # Ensure mask is on the same device as the module's weight
                module.weight.data.mul_(mask.to(module.weight.data.device))  # In-place masking
                linear_idx += 1
            else:
                print(f"Warning: Not enough masks for Linear layer {name} (index {linear_idx}). Skipping masking.")


    # Ensure all parameters are trainable (although they likely are unless set otherwise)
    for param in model.parameters():
        param.requires_grad = True

    return model

get_model_layerwise_analysis(pruned_model)

remaining_parameters = count_nonzeros(pruned_model)
print('For Compression without loss...')
print('Number of Initial Parameters are ' + str(initial_parameters))
print('Number of Remaining Parameters are ' + str(remaining_parameters))
print('Compression Rate without loss is ' + str(initial_parameters/remaining_parameters))
print('Best Accuracy is ' + str(round(best_accuracy*100,2)))
print('Best Loss is ' + str(round(best_loss,3)))

def layer_wise_comparison(model,pruned_model):
  selected_layers = [0,2,4,6,8,10,12,14]
  model_layers = []
  for param in model.parameters():
    if param is not None: model_layers.append(torch.sum((param != 0).int()).item())

  pruned_model_layers = []
  for param in pruned_model.parameters():
    if param is not None: pruned_model_layers.append(torch.sum((param != 0).int()).item())

  for i in range(len(model_layers)):
    if i in selected_layers:
      print('Layer Parameters: ' + str(model_layers[i]) + ', Parameters Left: ' + str(pruned_model_layers[i]) + ', Percentage Left: ' + str(round(pruned_model_layers[i]*100/model_layers[i],2)))

print('For Compression without loss...')
layer_wise_comparison(model,pruned_model)

print('For Best Compression...')
layer_wise_comparison(model,final_model)

print('Trained for '+str(last_parameter_size)+' non-zero parameters reaching '+str(initial_parameters/last_parameter_size)+' times compression with loss '+str(round(last_loss,3))+' and accuracy '+str(round(last_accuracy*100,2))+'%.')
